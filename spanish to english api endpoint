import os
import traceback

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import T5ForConditionalGeneration, T5Tokenizer

# add this:
from langdetect import detect, LangDetectException

# ----------------------------------------------------------------------------
# 1. DEVICE and MODEL PATHS
# ----------------------------------------------------------------------------
os.environ['CUDA_VISIBLE_DEVICES'] = '3'
LORA_ADAPTER_PATH = os.getenv(
    "LORA_PATH",
    "/app/cortex/dev1/aptaiModels/madlad_merged_LORA_8_v6"
)

# ----------------------------------------------------------------------------
# 2. LOAD TOKENIZER & MODEL
# ----------------------------------------------------------------------------
try:
    tokenizer = T5Tokenizer.from_pretrained(LORA_ADAPTER_PATH)
    base_model = T5ForConditionalGeneration.from_pretrained(LORA_ADAPTER_PATH)
    # base_model.to("cuda")
    base_model.eval()
except Exception as e:
    raise RuntimeError(f"Error loading model/tokenizer: {e}")

# ----------------------------------------------------------------------------
# 3. FASTAPI SETUP
# ----------------------------------------------------------------------------
app = FastAPI(
    title="Spanish→English Translation",
    description="Only translates Spanish to English. Rejects non-Spanish inputs.",
    version="1.0.0",
)

class TranslateRequest(BaseModel):
    input: str
    source: str  # must be "es"
    target: str  # must be "en"

class TranslateResponse(BaseModel):
    output: str

@app.get("/")
async def health_check():
    ok = tokenizer is not None and base_model is not None
    return {
        "status": "healthy" if ok else "failed",
        "lora_adapter": LORA_ADAPTER_PATH
    }

# ----------------------------------------------------------------------------
# 4. TRANSLATION ENDPOINT WITH VALIDATION
# ----------------------------------------------------------------------------
@app.post("/translate/", response_model=TranslateResponse)
async def translate(req: TranslateRequest):
    text = req.input.strip()
    if not text:
        raise HTTPException(400, detail="Input text must be non-empty")

    # enforce Spanish→English only
    if req.source.lower() != "es" or req.target.lower() != "en":
        raise HTTPException(
            400,
            detail="Only Spanish-to-English supported: source must be 'es' and target must be 'en'"
        )

    # detect language
    try:
        lang = detect(text)
    except LangDetectException:
        raise HTTPException(400, detail="Could not detect input language")

    if lang != "es":
        raise HTTPException(400, detail="Input text should be Spanish text")

    # build your fine-tuning prefix
    prefix = "translate es to en: "
    prompt = prefix + text

    try:
        inputs = tokenizer(prompt, return_tensors="pt").input_ids.to(base_model.device)
        outputs = base_model.generate(input_ids=inputs)
        translated = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return TranslateResponse(output=translated)
    except Exception as e:
        tb = traceback.format_exc()
        # (optionally log tb)
        raise HTTPException(500, detail=f"Translation failed: {e}")

# ----------------------------------------------------------------------------
# 5. RUN WITH UVICORN
# ----------------------------------------------------------------------------
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "translation_app:app",
        host="0.0.0.0",
        port=8100,
        reload=True
    )